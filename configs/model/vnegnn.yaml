_target_: src.wrappers.bindingsites.BindingSitesWrapper
_recursive_: false
compile: true
scaling_factor: 5.0
threshold: 4
monitor: val/dcc_ranked
interval: epoch
frequency: ${trainer.check_val_every_n_epoch}
cluster: null

loss:
  _target_: src.wrappers.bindingsites.BindingSitesLoss
  global_node_pos_loss:
    _target_: src.modules.losses.HuberLoss
  confidence_loss:
    _target_: src.modules.losses.ConfidenceLoss
  segmentation_loss:
    _target_: src.modules.losses.DiceLoss
  confidence_loss_weight: 1.0
  segmentation_loss_weight: 1.0

backbone:
  _target_: src.models.vnegnn.VNEGNN
  input_features: 1301
  node_features: 100
  edge_features: 1
  hidden_features: 100
  out_features: 100
  num_layers: 5
  dropout: 0.1
  residual: true

  act:
    _target_: torch.nn.SiLU
    _partial_: true

  node_aggr:
    _target_: torch_geometric.nn.MeanAggregation
    _partial_: true

  cord_aggr:
    _target_: torch_geometric.nn.MeanAggregation
    _partial_: true

  norm_coords: true
  norm_coors_scale_init: 0.01
  norm_feats: true

  initialization_gain: 1.0
  weight_share: false

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0001

ema: null
# ema:
#   _target_: src.modules.ema.ExponentialMovingAverage
#   _partial_: True
#   decay: 0.999

# scheduler:
#   _target_: src.modules.schedulers.LinearWarmupCosineAnnealingLR
#   _partial_: True
#   warmup_epochs: 0
#   max_epochs: ${trainer.max_epochs}
#   min_lr: 1e-6

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   _partial_: true
#   mode: max
#   patience: 100
